"""
LangGraph Agent for Experiment Notes Search
å®Ÿé¨“ãƒãƒ¼ãƒˆæ¤œç´¢ç”¨ã®LangGraphã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã‚’å‹•çš„ã«è¨­å®šå¯èƒ½
"""
import operator
import json
import re
import time
from typing import TypedDict, List, Annotated, Optional

from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.messages import HumanMessage, BaseMessage
import cohere

from config import config
from utils import load_master_dict, normalize_text
from prompts import get_default_prompt
from synonym_dictionary import get_synonym_dictionary
from chroma_sync import (
    get_chroma_vectorstore,
    get_team_chroma_vectorstore,
    get_team_multi_collection_vectorstores
)


# --- Stateå®šç¾© ---
class AgentState(TypedDict):
    messages: Annotated[List[BaseMessage], operator.add]

    # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
    input_purpose: str
    input_materials: str
    input_methods: str

    # å‡¦ç†ãƒ‡ãƒ¼ã‚¿
    normalized_materials: str
    user_focus_instruction: str
    search_query: str

    # æ¤œç´¢çµæœ
    retrieved_docs: List[str]  # UIè¡¨ç¤ºç”¨ã®æœ€çµ‚é¸æŠœï¼ˆé€šå¸¸: Top 3ã€è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰: Top 10ï¼‰

    iteration: int
    evaluation_mode: bool  # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ãƒ•ãƒ©ã‚°ï¼ˆTrue: æ¯”è¼ƒçœç•¥ã€Top10è¿”å´ï¼‰

    # v3.0.1: æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰è¨­å®š
    search_mode: str  # "semantic" | "keyword" | "hybrid"
    hybrid_alpha: float  # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é‡ã¿ï¼ˆ0.0-1.0ï¼‰

    # v3.1.0: 3è»¸åˆ†é›¢æ¤œç´¢è¨­å®š
    multi_axis_enabled: bool  # 3è»¸æ¤œç´¢ã®æœ‰åŠ¹/ç„¡åŠ¹
    focus_classification: str  # é‡ç‚¹æŒ‡ç¤ºã®åˆ†é¡çµæœ ("materials" | "methods" | "both" | "none")
    fusion_method: str  # ã‚¹ã‚³ã‚¢çµ±åˆæ–¹å¼ ("rrf" | "linear")
    axis_weights: dict  # å„è»¸ã®ã‚¦ã‚¨ã‚¤ãƒˆ {"material": 0.3, "method": 0.4, "combined": 0.3}
    rerank_position: str  # ãƒªãƒ©ãƒ³ã‚¯ä½ç½® ("per_axis" | "after_fusion")
    rerank_enabled: bool  # ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®æœ‰åŠ¹/ç„¡åŠ¹

    # 3è»¸æ¤œç´¢çµæœ
    material_query: str  # ææ–™è»¸ã‚¯ã‚¨ãƒª
    method_query: str  # æ–¹æ³•è»¸ã‚¯ã‚¨ãƒª
    combined_query: str  # ç·åˆè»¸ã‚¯ã‚¨ãƒª
    material_axis_results: List[tuple]  # ææ–™è»¸ã®æ¤œç´¢çµæœ [(doc, score), ...]
    method_axis_results: List[tuple]  # æ–¹æ³•è»¸ã®æ¤œç´¢çµæœ [(doc, score), ...]
    combined_axis_results: List[tuple]  # ç·åˆè»¸ã®æ¤œç´¢çµæœ [(doc, score), ...]


class SearchAgent:
    """æ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ»ãƒ¢ãƒ‡ãƒ«ã‚’å‹•çš„è¨­å®šå¯èƒ½ï¼‰"""

    def __init__(
        self,
        openai_api_key: str,
        cohere_api_key: str,
        embedding_model: str = None,
        llm_model: str = None,  # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ç¶­æŒ
        search_llm_model: str = None,  # v3.0: æ¤œç´¢ãƒ»åˆ¤å®šç”¨LLM
        summary_llm_model: str = None,  # v3.0: è¦ç´„ç”Ÿæˆç”¨LLM
        search_mode: str = None,  # v3.0.1: æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰
        hybrid_alpha: float = None,  # v3.0.1: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®é‡ã¿
        prompts: dict = None,
        team_id: str = None,  # v3.0: ãƒãƒ«ãƒãƒ†ãƒŠãƒ³ãƒˆå¯¾å¿œ
        # v3.1.0: 3è»¸åˆ†é›¢æ¤œç´¢è¨­å®š
        multi_axis_enabled: bool = None,
        fusion_method: str = None,
        axis_weights: dict = None,
        rerank_position: str = None,
        rerank_enabled: bool = None
    ):
        """
        Args:
            openai_api_key: OpenAI APIã‚­ãƒ¼
            cohere_api_key: Cohere APIã‚­ãƒ¼
            embedding_model: Embeddingãƒ¢ãƒ‡ãƒ«å
            llm_model: LLMãƒ¢ãƒ‡ãƒ«åï¼ˆå¾Œæ–¹äº’æ›æ€§ã€éæ¨å¥¨ï¼‰
            search_llm_model: æ¤œç´¢ãƒ»åˆ¤å®šç”¨LLMãƒ¢ãƒ‡ãƒ«åï¼ˆv3.0ï¼‰
            summary_llm_model: è¦ç´„ç”Ÿæˆç”¨LLMãƒ¢ãƒ‡ãƒ«åï¼ˆv3.0ï¼‰
            search_mode: æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ï¼ˆv3.0.1ï¼‰"semantic" | "keyword" | "hybrid"
            hybrid_alpha: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é‡ã¿ï¼ˆv3.0.1ï¼‰0.0-1.0
            prompts: ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¾æ›¸
            team_id: ãƒãƒ¼ãƒ IDï¼ˆv3.0ï¼‰
            multi_axis_enabled: 3è»¸æ¤œç´¢ã®æœ‰åŠ¹/ç„¡åŠ¹ï¼ˆv3.1.0ï¼‰
            fusion_method: ã‚¹ã‚³ã‚¢çµ±åˆæ–¹å¼ï¼ˆv3.1.0ï¼‰"rrf" | "linear"
            axis_weights: å„è»¸ã®ã‚¦ã‚¨ã‚¤ãƒˆï¼ˆv3.1.0ï¼‰{"material": 0.3, "method": 0.4, "combined": 0.3}
            rerank_position: ãƒªãƒ©ãƒ³ã‚¯ä½ç½®ï¼ˆv3.1.0ï¼‰"per_axis" | "after_fusion"
            rerank_enabled: ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®æœ‰åŠ¹/ç„¡åŠ¹ï¼ˆv3.1.0ï¼‰
        """
        self.openai_api_key = openai_api_key
        self.cohere_api_key = cohere_api_key
        self.team_id = team_id

        # ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆv3.0: 2æ®µéšé¸æŠå¯¾å¿œï¼‰
        self.embedding_model = embedding_model or config.DEFAULT_EMBEDDING_MODEL
        # å¾Œæ–¹äº’æ›æ€§: llm_modelãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°ãã‚Œã‚’ä½¿ç”¨
        self.search_llm_model = search_llm_model or llm_model or config.DEFAULT_SEARCH_LLM_MODEL
        self.summary_llm_model = summary_llm_model or llm_model or config.DEFAULT_SUMMARY_LLM_MODEL

        # æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰è¨­å®šï¼ˆv3.0.1ï¼‰
        self.search_mode = search_mode or config.DEFAULT_SEARCH_MODE
        self.hybrid_alpha = hybrid_alpha if hybrid_alpha is not None else config.DEFAULT_HYBRID_ALPHA

        # 3è»¸åˆ†é›¢æ¤œç´¢è¨­å®šï¼ˆv3.1.0ï¼‰
        self.multi_axis_enabled = multi_axis_enabled if multi_axis_enabled is not None else config.MULTI_AXIS_ENABLED
        self.fusion_method = fusion_method or config.FUSION_METHOD
        self.axis_weights = axis_weights or config.AXIS_WEIGHTS
        self.rerank_position = rerank_position or config.RERANK_POSITION
        self.rerank_enabled = rerank_enabled if rerank_enabled is not None else config.RERANK_ENABLED

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šï¼ˆã‚«ã‚¹ã‚¿ãƒ ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        self.prompts = prompts or {}

        # Cohere ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        self.cohere_client = cohere.Client(cohere_api_key)

        # æ­£è¦åŒ–è¾æ›¸
        self.norm_map, _ = load_master_dict()

        # åŒç¾©èªè¾æ›¸ï¼ˆv3.2.1: ã‚¯ã‚¨ãƒªå±•é–‹ç”¨ï¼‰
        self.synonym_dict = get_synonym_dictionary(team_id)

        # Embeddingé–¢æ•°
        self.embedding_function = OpenAIEmbeddings(
            model=self.embedding_model,
            api_key=self.openai_api_key
        )

        # Vector Storeï¼ˆv3.2.0: 2ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å¯¾å¿œã«å¤‰æ›´ã€v3.2.2: å¸¸ã«2ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å–å¾—ã«å¤‰æ›´ï¼‰
        if team_id:
            # ãƒãƒ¼ãƒ ãƒ¢ãƒ¼ãƒ‰: å¸¸ã«2ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–å¾—ï¼ˆ3è»¸æ¤œç´¢ã®æœ‰åŠ¹/ç„¡åŠ¹ã«é–¢ã‚ã‚‰ãšï¼‰
            # ã“ã‚Œã«ã‚ˆã‚Šã€3è»¸æ¤œç´¢ãŒç„¡åŠ¹ã§ã‚‚combinedã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã§æ¤œç´¢å¯èƒ½
            self.vectorstores = get_team_multi_collection_vectorstores(
                team_id=team_id,
                embeddings=self.embedding_function,
                embedding_model=self.embedding_model
            )
            # vectorstoreã¯combinedã‚’å‚ç…§ï¼ˆå˜ä¸€ã‚¯ã‚¨ãƒªæ¤œç´¢æ™‚ã«ä½¿ç”¨ï¼‰
            self.vectorstore = self.vectorstores["combined"]
            if self.multi_axis_enabled:
                print(f"2ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ï¼ˆ3è»¸æ¤œç´¢æœ‰åŠ¹ï¼‰: materials_methods, combined vectorstoresåˆæœŸåŒ–å®Œäº†")
            else:
                print(f"2ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãƒ¢ãƒ¼ãƒ‰ï¼ˆå˜ä¸€ã‚¯ã‚¨ãƒªæ¤œç´¢ï¼‰: combinedã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨")
        else:
            # å¾Œæ–¹äº’æ›æ€§: team_idãŒãªã„å ´åˆã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚’ä½¿ç”¨
            self.vectorstores = None
            self.vectorstore = get_chroma_vectorstore(
                self.embedding_function,
                embedding_model=self.embedding_model
            )

        # LLMï¼ˆv3.0: 2æ®µéšé¸æŠå¯¾å¿œï¼‰
        # temperatureã‚’ã‚µãƒãƒ¼ãƒˆã—ãªã„ãƒ¢ãƒ‡ãƒ«ã®åˆ¤å®š
        def supports_temperature(model_name: str) -> bool:
            """temperatureãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‹ã©ã†ã‹åˆ¤å®š"""
            no_temp_models = ['o1', 'o1-mini', 'o1-preview', 'o3-mini', 'gpt-5-mini', 'gpt-5-nano']
            return not any(m in model_name for m in no_temp_models)

        # æ¤œç´¢ãƒ»åˆ¤å®šç”¨LLMï¼ˆæ­£è¦åŒ–ã€ã‚¯ã‚¨ãƒªç”Ÿæˆã«ä½¿ç”¨ï¼‰
        search_llm_kwargs = {
            "model": self.search_llm_model,
            "api_key": self.openai_api_key,
            "seed": 42  # v3.2.4: å†ç¾æ€§ã®ãŸã‚seedã‚’å›ºå®š
        }
        if supports_temperature(self.search_llm_model):
            search_llm_kwargs["temperature"] = 0
        self.search_llm = ChatOpenAI(**search_llm_kwargs)

        # è¦ç´„ç”Ÿæˆç”¨LLMï¼ˆæ¯”è¼ƒãƒãƒ¼ãƒ‰ã«ä½¿ç”¨ï¼‰
        summary_llm_kwargs = {
            "model": self.summary_llm_model,
            "api_key": self.openai_api_key,
            "seed": 42  # v3.2.4: å†ç¾æ€§ã®ãŸã‚seedã‚’å›ºå®š
        }
        if supports_temperature(self.summary_llm_model):
            summary_llm_kwargs["temperature"] = 0
        self.summary_llm = ChatOpenAI(**summary_llm_kwargs)
        # å¾Œæ–¹äº’æ›æ€§: self.llmã¯search_llmã‚’å‚ç…§
        self.llm = self.search_llm

        # ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
        self.graph = self._build_graph()

    def _get_prompt(self, prompt_type: str) -> str:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—ï¼ˆã‚«ã‚¹ã‚¿ãƒ ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰

        ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒç©ºæ–‡å­—ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨ã™ã‚‹
        """
        custom = self.prompts.get(prompt_type)
        # ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒå­˜åœ¨ã—ã€ç©ºæ–‡å­—ã§ãªã„å ´åˆã®ã¿ä½¿ç”¨
        if custom and custom.strip():
            return custom
        return get_default_prompt(prompt_type)

    def _normalize_node(self, state: AgentState):
        """æ­£è¦åŒ–ãƒãƒ¼ãƒ‰"""
        start_time = time.time()
        evaluation_mode = state.get("evaluation_mode", False)

        if evaluation_mode:
            print("\n" + "="*80)
            print("ğŸ”¬ [è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰] æ€§èƒ½è©•ä¾¡å®Ÿè¡Œä¸­")
            print("="*80)
            print("\n--- ğŸš€ [1/3] æ­£è¦åŒ– & JSONè§£æ ---")
        else:
            print("\n--- ğŸš€ [1/4] æ­£è¦åŒ– & JSONè§£æ ---")

        updates = {}
        messages = state.get("messages", [])

        # JSONè§£æ
        if messages:
            last_msg = messages[-1]
            content = ""
            if hasattr(last_msg, "content"):
                content = last_msg.content
            elif isinstance(last_msg, dict):
                content = last_msg.get("content", "")
            else:
                content = str(last_msg)

            if content.strip().startswith("{"):
                try:
                    data = json.loads(content)

                    if data.get("type") == "initial_search":
                        updates["input_purpose"] = data.get("purpose", "")
                        updates["input_materials"] = data.get("materials", "")
                        updates["input_methods"] = data.get("methods", "")

                        # v3.2.4: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé‡ç‚¹æŒ‡ç¤ºã‚’å…¥åŠ›ã—ã¦ã„ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€
                        # ç©ºã®å ´åˆã®ã¿ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæŒ‡ç¤ºã‚’é©ç”¨
                        user_instruction = data.get("instruction", "").strip()
                        if user_instruction:
                            updates["user_focus_instruction"] = user_instruction
                            print(f"  ğŸ“Œ é‡ç‚¹æŒ‡ç¤ºï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šï¼‰: {user_instruction[:50]}...")
                        else:
                            updates["user_focus_instruction"] = (
                                "ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ææ–™(åŒ–å­¦ç‰©è³ªã€å®¹é‡ï¼‰ã¨ã€æ–¹æ³•ï¼ˆåŒ–å­¦ç‰©è³ªã€å®¹é‡ã€æ‰‹é †ï¼‰ã®è¨˜è¿°ãŒ"
                                "é¡ä¼¼ã—ã¦ã„ã‚‹å®Ÿé¨“ãƒãƒ¼ãƒˆã‚’æœ€å„ªå…ˆã—ã¦æ¤œç´¢ã—ã¦ãã ã•ã„ã€‚"
                            )
                            print(f"  ğŸ“Œ é‡ç‚¹æŒ‡ç¤ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé©ç”¨ï¼‰")

                    elif data.get("type") == "refinement":
                        updates["user_focus_instruction"] = data.get("instruction", "")
                        updates["input_purpose"] = data.get("purpose", "")
                        updates["input_materials"] = data.get("materials", "")
                        updates["input_methods"] = data.get("methods", "")

                except json.JSONDecodeError:
                    print("  > âš ï¸ JSON Decode Error")

        # æ­£è¦åŒ–å‡¦ç†
        raw_materials = updates.get("input_materials", state.get("input_materials", ""))
        normalized_parts = []

        if raw_materials:
            lines = raw_materials.split('\n')
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                parts = re.split(r'[:ï¼š]', line, 1)

                if len(parts) == 2:
                    left_part = parts[0]
                    amount_part = parts[1]
                    raw_name = re.sub(r'^[-ãƒ»\s]*[â‘ -â‘¨0-9.]*\s*', '', left_part).strip()
                    norm_name = normalize_text(raw_name, self.norm_map)
                    normalized_parts.append(f"- {norm_name}: {amount_part.strip()}")
                else:
                    clean_line = re.sub(r'^[-ãƒ»\s]*[â‘ -â‘¨0-9.]*\s*', '', line).strip()
                    norm_name = normalize_text(clean_line, self.norm_map)
                    normalized_parts.append(norm_name)

        normalized_str = "\n".join(normalized_parts) if normalized_parts else raw_materials
        updates["normalized_materials"] = normalized_str

        # æ­£è¦åŒ–å®Œäº†ã‚’ã‚µãƒãƒªè¡¨ç¤ºï¼ˆæ¤œç´¢/è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰å…±é€šï¼‰
        material_count = len(normalized_parts) if normalized_parts else 0
        print(f"  ğŸ“ æ­£è¦åŒ–å®Œäº†: {material_count}ææ–™")

        elapsed_time = time.time() - start_time
        print(f"  â±ï¸ Execution Time: {elapsed_time:.4f} sec")
        return updates

    def _generate_query_node(self, state: AgentState):
        """ã‚¯ã‚¨ãƒªç”Ÿæˆãƒãƒ¼ãƒ‰ï¼ˆå˜ä¸€ã‚¯ã‚¨ãƒªæ¤œç´¢æ™‚ï¼‰

        v3.2.3: combined_query_generationãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«å¤‰æ›´
        3è»¸åˆ†é›¢æ¤œç´¢ã¨åŒã˜ã€Œç·åˆè»¸ã‚¯ã‚¨ãƒªç”Ÿæˆã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ã†ã“ã¨ã§ã€
        ä¿å­˜æ¸ˆã¿ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒæœ‰åŠ¹ã«ãªã‚‹
        """
        start_time = time.time()
        evaluation_mode = state.get("evaluation_mode", False)

        if evaluation_mode:
            print("\n--- ğŸ§  [2/3] ç·åˆè»¸ã‚¯ã‚¨ãƒªç”Ÿæˆï¼ˆå˜ä¸€ã‚¯ã‚¨ãƒªãƒ¢ãƒ¼ãƒ‰ï¼‰---")
        else:
            print("--- ğŸ§  [2/4] ç·åˆè»¸ã‚¯ã‚¨ãƒªç”Ÿæˆï¼ˆå˜ä¸€ã‚¯ã‚¨ãƒªãƒ¢ãƒ¼ãƒ‰ï¼‰---")

        instruction = state.get('user_focus_instruction', 'ç‰¹ã«ãªã—')

        # v3.2.3: combined_query_generationãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
        # ã“ã‚Œã«ã‚ˆã‚Šã€3è»¸åˆ†é›¢æ¤œç´¢ã¨åŒã˜ã€Œç·åˆè»¸ã‚¯ã‚¨ãƒªç”Ÿæˆã€ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒé©ç”¨ã•ã‚Œã‚‹
        prompt_template = self._get_prompt("combined_query_generation")

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¤‰æ•°ã‚’åŸ‹ã‚è¾¼ã‚€ï¼ˆNoneã®å ´åˆã¯ç©ºæ–‡å­—åˆ—ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        prompt = prompt_template.format(
            input_purpose=state.get('input_purpose') or '',
            normalized_materials=state.get('normalized_materials') or '',
            input_methods=state.get('input_methods') or '',
            user_focus_instruction=instruction
        )

        response = self.llm.invoke(prompt)

        content = response.content.strip()

        # JSONã‚’æŠ½å‡ºï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ–ãƒ­ãƒƒã‚¯ã€ä½™è¨ˆãªãƒ†ã‚­ã‚¹ãƒˆã«å¯¾å¿œï¼‰
        def extract_json(text: str) -> str:
            """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰JSONéƒ¨åˆ†ã‚’æŠ½å‡º"""
            import re
            # ```json ... ``` ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¢ã™
            json_block = re.search(r'```(?:json)?\s*([\s\S]*?)```', text)
            if json_block:
                return json_block.group(1).strip()
            # { ... } ã‚’æ¢ã™
            json_match = re.search(r'\{[\s\S]*\}', text)
            if json_match:
                return json_match.group(0)
            return text

        content = extract_json(content)

        try:
            data = json.loads(content)
            queries = data.get("queries", [])
            if not queries:
                raise ValueError("Empty queries")

            combined_query = " ".join(queries)

            # ã‚¯ã‚¨ãƒªå…¨ä½“ã‚’è¡¨ç¤ºï¼ˆæ¤œç´¢/è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰å…±é€šï¼‰
            print(f"\n  ğŸ” [ç”Ÿæˆã•ã‚ŒãŸã‚¯ã‚¨ãƒª] ({len(queries)}å€‹)")
            for i, q in enumerate(queries, 1):
                print(f"    {i}. {q}")
            print(f"\n  ğŸ“ [çµ±åˆæ¤œç´¢ã‚¯ã‚¨ãƒª]\n    {combined_query}")

        except Exception as e:
            print(f"  > âš ï¸ Query Parse Error: {e}")
            print(f"  > Raw response: {response.content[:200]}...")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…¥åŠ›ã‚’ãã®ã¾ã¾ã‚¯ã‚¨ãƒªã¨ã—ã¦ä½¿ç”¨
            combined_query = f"{state.get('input_purpose') or ''} {state.get('normalized_materials') or ''} {instruction}"
            print(f"  > Fallback query: {combined_query[:100]}...")

        elapsed_time = time.time() - start_time
        print(f"  â±ï¸ Execution Time: {elapsed_time:.4f} sec")
        return {"search_query": combined_query}

    def _expand_query_with_synonyms(self, query: str) -> List[str]:
        """åŒç¾©èªè¾æ›¸ã‚’ä½¿ã£ã¦ã‚¯ã‚¨ãƒªã‚’å±•é–‹ï¼ˆv3.2.1ï¼‰

        Args:
            query: å…ƒã®ã‚¯ã‚¨ãƒª

        Returns:
            å±•é–‹ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã®ãƒªã‚¹ãƒˆï¼ˆå…ƒã®ã‚¯ã‚¨ãƒªã‚’å«ã‚€ï¼‰
        """
        return self.synonym_dict.expand_query(query)

    def _search_with_synonym_expansion(
        self,
        vectorstore,
        query: str,
        search_mode: str,
        hybrid_alpha: float,
        k: int = 30
    ) -> List[tuple]:
        """åŒç¾©èªå±•é–‹ã‚’é©ç”¨ã—ãŸæ¤œç´¢ï¼ˆv3.2.1ï¼‰

        è¤‡æ•°ã®ã‚¯ã‚¨ãƒªã§æ¤œç´¢ã—ã€çµæœã‚’ãƒãƒ¼ã‚¸ã™ã‚‹ã€‚

        Args:
            vectorstore: æ¤œç´¢å¯¾è±¡ã®vectorstore
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            search_mode: æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰
            hybrid_alpha: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®é‡ã¿
            k: è¿”å´ã™ã‚‹ä¸Šä½ä»¶æ•°

        Returns:
            List of (doc, score) tuples
        """
        # ã‚¯ã‚¨ãƒªã‚’åŒç¾©èªå±•é–‹
        expanded_queries = self._expand_query_with_synonyms(query)

        if len(expanded_queries) > 1:
            print(f"    > åŒç¾©èªå±•é–‹: {len(expanded_queries)}ã‚¯ã‚¨ãƒªã«å±•é–‹")

        # å„ã‚¯ã‚¨ãƒªã§æ¤œç´¢ã—ã€çµæœã‚’ãƒãƒ¼ã‚¸
        all_results = {}  # {note_id: (doc, max_score)}

        for eq in expanded_queries:
            # æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ãŸæ¤œç´¢å®Ÿè¡Œ
            if search_mode == "keyword":
                results = self._keyword_search_on_vectorstore(vectorstore, eq, k=k)
            elif search_mode == "hybrid":
                results = self._hybrid_search_on_vectorstore(vectorstore, eq, alpha=hybrid_alpha, k=k)
            else:
                # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢
                docs = vectorstore.similarity_search_with_relevance_scores(eq, k=k)
                results = [(doc, score) for doc, score in docs]

            # çµæœã‚’ãƒãƒ¼ã‚¸ï¼ˆåŒã˜ãƒãƒ¼ãƒˆã¯æœ€é«˜ã‚¹ã‚³ã‚¢ã‚’æ¡ç”¨ï¼‰
            for doc, score in results:
                note_id = doc.metadata.get('note_id', doc.metadata.get('source', doc.page_content[:50]))
                if note_id not in all_results or score > all_results[note_id][1]:
                    all_results[note_id] = (doc, score)

        # ã‚¹ã‚³ã‚¢é™é †ã§ã‚½ãƒ¼ãƒˆ
        merged_results = list(all_results.values())
        merged_results.sort(key=lambda x: x[1], reverse=True)
        return merged_results[:k]

    def _keyword_search(self, query: str, k: int = 30) -> List[tuple]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆBM25ãƒ™ãƒ¼ã‚¹ï¼‰

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: è¿”å´ã™ã‚‹ä¸Šä½ä»¶æ•°

        Returns:
            List of (doc, score) tuples sorted by score descending
        """
        import math
        from collections import Counter

        # ChromaDBã‹ã‚‰å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
        collection = self.vectorstore._collection
        all_docs = collection.get(include=["documents", "metadatas"])

        if not all_docs["documents"]:
            return []

        documents = all_docs["documents"]
        metadatas = all_docs["metadatas"]

        # ã‚¯ã‚¨ãƒªã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆç°¡æ˜“çš„ãªæ—¥æœ¬èªå¯¾å¿œï¼‰
        query_tokens = self._tokenize(query)

        # BM25ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        k1 = 1.5
        b = 0.75

        # æ–‡æ›¸é•·ã®å¹³å‡ã‚’è¨ˆç®—
        doc_lengths = [len(self._tokenize(doc)) for doc in documents]
        avgdl = sum(doc_lengths) / len(doc_lengths) if doc_lengths else 1

        # IDFè¨ˆç®—
        N = len(documents)
        idf = {}
        for token in query_tokens:
            df = sum(1 for doc in documents if token in doc.lower())
            idf[token] = math.log((N - df + 0.5) / (df + 0.5) + 1)

        # BM25ã‚¹ã‚³ã‚¢è¨ˆç®—
        scores = []
        for i, doc in enumerate(documents):
            doc_tokens = self._tokenize(doc)
            doc_len = len(doc_tokens)
            term_freq = Counter(doc_tokens)

            score = 0
            for token in query_tokens:
                if token in term_freq:
                    tf = term_freq[token]
                    numerator = tf * (k1 + 1)
                    denominator = tf + k1 * (1 - b + b * doc_len / avgdl)
                    score += idf.get(token, 0) * numerator / denominator

            # Document objectã‚’ä½œæˆ
            from langchain_core.documents import Document
            doc_obj = Document(
                page_content=doc,
                metadata=metadatas[i] if metadatas else {}
            )
            scores.append((doc_obj, score))

        # ã‚¹ã‚³ã‚¢é™é †ã§ã‚½ãƒ¼ãƒˆ
        scores.sort(key=lambda x: x[1], reverse=True)
        return scores[:k]

    def _tokenize(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆç°¡æ˜“çš„ãªæ—¥æœ¬èªå¯¾å¿œï¼‰

        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆ
        """
        # å°æ–‡å­—åŒ–
        text = text.lower()

        # æ—¥æœ¬èªã¨è‹±èªã‚’åˆ†é›¢ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        # ç°¡æ˜“çš„ã«ç©ºç™½ã€å¥èª­ç‚¹ã€ç‰¹æ®Šæ–‡å­—ã§åˆ†å‰²
        import re
        # æ—¥æœ¬èªã®å ´åˆã¯æ–‡å­—å˜ä½ã§2-gramã‚’ç”Ÿæˆ
        tokens = []

        # è‹±æ•°å­—ã®å˜èªã‚’æŠ½å‡º
        words = re.findall(r'[a-z0-9]+', text)
        tokens.extend(words)

        # æ—¥æœ¬èªéƒ¨åˆ†ã‚’æŠ½å‡ºï¼ˆã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠã€æ¼¢å­—ï¼‰
        japanese_text = re.sub(r'[a-z0-9\s\.,!?:;()\[\]{}\-_]+', '', text)
        # 2-gramã§åˆ†å‰²ï¼ˆã‚ˆã‚Šç²¾åº¦ã®é«˜ã„ãƒãƒƒãƒãƒ³ã‚°ã®ãŸã‚ï¼‰
        for i in range(len(japanese_text) - 1):
            tokens.append(japanese_text[i:i+2])
        # 1-gramã‚‚è¿½åŠ 
        tokens.extend(list(japanese_text))

        return tokens

    def _hybrid_search(self, query: str, alpha: float, k: int = 30) -> List[tuple]:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ + ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            alpha: ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã®é‡ã¿ï¼ˆ0.0-1.0ï¼‰
            k: è¿”å´ã™ã‚‹ä¸Šä½ä»¶æ•°

        Returns:
            List of (doc, score) tuples sorted by combined score descending
        """
        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢
        semantic_results = self.vectorstore.similarity_search_with_relevance_scores(query, k=k)

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
        keyword_results = self._keyword_search(query, k=k)

        # ã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ–ã¨çµ±åˆ
        doc_scores = {}

        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢çµæœã‚’ã‚¹ã‚³ã‚¢ä»˜ã‘
        if semantic_results:
            max_semantic = max(score for _, score in semantic_results)
            min_semantic = min(score for _, score in semantic_results)
            range_semantic = max_semantic - min_semantic if max_semantic != min_semantic else 1

            for doc, score in semantic_results:
                # 0-1ã«æ­£è¦åŒ–
                normalized_score = (score - min_semantic) / range_semantic if range_semantic > 0 else 0.5
                doc_id = doc.metadata.get('source', doc.page_content[:50])
                if doc_id not in doc_scores:
                    doc_scores[doc_id] = {'doc': doc, 'semantic': 0, 'keyword': 0}
                doc_scores[doc_id]['semantic'] = normalized_score

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢çµæœã‚’ã‚¹ã‚³ã‚¢ä»˜ã‘
        if keyword_results:
            max_keyword = max(score for _, score in keyword_results)
            min_keyword = min(score for _, score in keyword_results)
            range_keyword = max_keyword - min_keyword if max_keyword != min_keyword else 1

            for doc, score in keyword_results:
                # 0-1ã«æ­£è¦åŒ–
                normalized_score = (score - min_keyword) / range_keyword if range_keyword > 0 else 0.5
                doc_id = doc.metadata.get('source', doc.page_content[:50])
                if doc_id not in doc_scores:
                    doc_scores[doc_id] = {'doc': doc, 'semantic': 0, 'keyword': 0}
                doc_scores[doc_id]['keyword'] = normalized_score

        # çµ±åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        combined_results = []
        for doc_id, scores in doc_scores.items():
            combined_score = alpha * scores['semantic'] + (1 - alpha) * scores['keyword']
            combined_results.append((scores['doc'], combined_score))

        # ã‚¹ã‚³ã‚¢é™é †ã§ã‚½ãƒ¼ãƒˆ
        combined_results.sort(key=lambda x: x[1], reverse=True)
        return combined_results[:k]

    def _search_node(self, state: AgentState):
        """æ¤œç´¢ & Cohereãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒãƒ¼ãƒ‰ï¼ˆv3.0.1: æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼‰"""
        start_time = time.time()
        evaluation_mode = state.get("evaluation_mode", False)

        # æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã‚’å–å¾—ï¼ˆstateã‹ã‚‰ã€ã¾ãŸã¯ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã‹ã‚‰ï¼‰
        search_mode = state.get("search_mode", self.search_mode)
        hybrid_alpha = state.get("hybrid_alpha", self.hybrid_alpha)

        mode_label = {
            "semantic": "ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯",
            "keyword": "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆBM25ï¼‰",
            "hybrid": f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼ˆÎ±={hybrid_alpha:.2f}ï¼‰"
        }.get(search_mode, "ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯")

        if evaluation_mode:
            print(f"--- ğŸ” [3/3] {mode_label}æ¤œç´¢ & Cohereãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°å®Ÿè¡Œï¼ˆè©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ï¼‰---")
        else:
            print(f"--- ğŸ” [3/4] {mode_label}æ¤œç´¢ & Cohereãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°å®Ÿè¡Œ ---")

        query = state["search_query"]

        try:
            # ChromaDBã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã‚’ç¢ºèª
            collection = self.vectorstore._collection
            doc_count = collection.count()
            print(f"  > ChromaDB Collection: {doc_count} documents")
            print(f"  > Search Mode: {search_mode}")

            # v3.2.1: åŒç¾©èªå±•é–‹ã‚’é©ç”¨ã—ãŸæ¤œç´¢
            search_results = self._search_with_synonym_expansion(
                vectorstore=self.vectorstore,
                query=query,
                search_mode=search_mode,
                hybrid_alpha=hybrid_alpha,
                k=config.VECTOR_SEARCH_K
            )
            candidates = [doc for doc, score in search_results]
            print(f"  > Retrieved {len(candidates)} candidates (with synonym expansion).")

            if not candidates:
                print("  > No candidates found.")
                print(f"  â±ï¸ Execution Time: {time.time() - start_time:.4f} sec")
                return {"retrieved_docs": [], "iteration": state.get("iteration", 0) + 1}

            # Cohere Rerank
            documents_content = [doc.page_content for doc in candidates]

            rerank_results = self.cohere_client.rerank(
                model=config.DEFAULT_RERANK_MODEL,
                query=query,
                documents=documents_content,
                top_n=config.RERANK_TOP_N
            )

            if evaluation_mode:
                print(f"\n  ğŸ“Š [ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœ] Top {config.RERANK_TOP_N} ä»¶")
                print(f"  " + "="*76)
            else:
                print(f"\n  ğŸ“Š [Console Log] Top {config.RERANK_TOP_N} Cohere Rerank Results:")
                print(f"  --------------------------------------------------")

            docs_for_ui = []
            seen_source_ids = set()  # é‡è¤‡é™¤å»ç”¨

            # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ãªã‚‰å…¨ä»¶ï¼ˆTop10ï¼‰ã€é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ãªã‚‰ä¸Šä½3ä»¶ã®ã¿
            display_limit = config.RERANK_TOP_N if evaluation_mode else config.UI_DISPLAY_TOP_N

            rank_counter = 0  # é‡è¤‡é™¤å»å¾Œã®ãƒ©ãƒ³ã‚¯
            for i, result in enumerate(rerank_results.results):
                original_doc = candidates[result.index]
                source_id = original_doc.metadata.get('source', 'unknown')
                score = result.relevance_score
                snippet = original_doc.page_content[:50].replace('\n', ' ')

                # é‡è¤‡ãƒã‚§ãƒƒã‚¯: æ—¢ã«è¿½åŠ æ¸ˆã¿ã®ãƒãƒ¼ãƒˆIDã¯ã‚¹ã‚­ãƒƒãƒ—
                if source_id in seen_source_ids:
                    continue
                seen_source_ids.add(source_id)
                rank_counter += 1

                if evaluation_mode:
                    print(f"  Rank {rank_counter:2d} | Score: {score:.6f} | ãƒãƒ¼ãƒˆID: {source_id}")
                else:
                    print(f"  Rank {rank_counter:2d} | Score: {score:.4f} | ID: {source_id} | {snippet}...")

                # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ãªã‚‰å…¨ä»¶ã€é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ãªã‚‰ä¸Šä½3ä»¶ã®ã¿ä¿å­˜
                if rank_counter <= display_limit:
                    docs_for_ui.append(f"ã€å®Ÿé¨“ãƒãƒ¼ãƒˆID: {source_id}ã€‘\n{original_doc.page_content}")

            if evaluation_mode:
                print(f"  " + "="*76)
                print(f"  âœ… è©•ä¾¡ç”¨ã«ä¸Šä½ {len(docs_for_ui)} ä»¶ã‚’è¿”å´ã—ã¾ã™ã€‚")
            else:
                print(f"  --------------------------------------------------")
                print(f"  > UIå‘ã‘ã«ä¸Šä½ {len(docs_for_ui)} ä»¶ã‚’é¸æŠã—ã¾ã—ãŸã€‚")

        except Exception as e:
            print(f"  > âš ï¸ Search/Rerank Error: {e}")
            docs_for_ui = []

        elapsed_time = time.time() - start_time
        print(f"  â±ï¸ Execution Time: {elapsed_time:.4f} sec")

        # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰æ™‚ã¯çµ‚äº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        if evaluation_mode:
            print("\n" + "="*80)
            print("âœ… è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰çµ‚äº† - æ¯”è¼ƒãƒãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦çµæœã‚’è¿”å´ã—ã¾ã™")
            print("="*80 + "\n")

        return {
            "retrieved_docs": docs_for_ui,
            "iteration": state.get("iteration", 0) + 1
        }

    # ===========================================
    # v3.1.0: 3è»¸åˆ†é›¢æ¤œç´¢ç”¨ãƒãƒ¼ãƒ‰
    # ===========================================

    def _extract_json_from_response(self, text: str) -> str:
        """LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰JSONéƒ¨åˆ†ã‚’æŠ½å‡ºã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼"""
        # ```json ... ``` ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¢ã™
        json_block = re.search(r'```(?:json)?\s*([\s\S]*?)```', text)
        if json_block:
            return json_block.group(1).strip()
        # { ... } ã‚’æ¢ã™
        json_match = re.search(r'\{[\s\S]*\}', text)
        if json_match:
            return json_match.group(0)
        return text

    def _classify_focus_node(self, state: AgentState):
        """é‡ç‚¹æŒ‡ç¤ºåˆ†é¡ãƒãƒ¼ãƒ‰ï¼ˆv3.1.0ï¼‰

        é‡ç‚¹æŒ‡ç¤ºã‚’LLMã§è§£æã—ã€ææ–™/æ–¹æ³•/ä¸¡æ–¹/ãªã—ã‚’åˆ¤å®šã™ã‚‹
        """
        start_time = time.time()
        evaluation_mode = state.get("evaluation_mode", False)

        if evaluation_mode:
            print("\n--- ğŸ·ï¸ [2/6] é‡ç‚¹æŒ‡ç¤ºåˆ†é¡ ---")
        else:
            print("--- ğŸ·ï¸ [2/7] é‡ç‚¹æŒ‡ç¤ºåˆ†é¡ ---")

        instruction = state.get('user_focus_instruction', '')

        # é‡ç‚¹æŒ‡ç¤ºãŒç©ºã®å ´åˆã¯"none"
        if not instruction or instruction.strip() in ['', 'ç‰¹ã«ãªã—', 'ãªã—']:
            print(f"  > é‡ç‚¹æŒ‡ç¤ºãŒç©ºã®ãŸã‚ã€åˆ†é¡ã‚’ã‚¹ã‚­ãƒƒãƒ—: none")
            elapsed_time = time.time() - start_time
            print(f"  â±ï¸ Execution Time: {elapsed_time:.4f} sec")
            return {"focus_classification": "none"}

        # LLMã§åˆ†é¡
        prompt_template = self._get_prompt("focus_classification")
        prompt = prompt_template.format(user_focus_instruction=instruction)

        try:
            response = self.search_llm.invoke(prompt)
            content = self._extract_json_from_response(response.content.strip())
            data = json.loads(content)
            classification = data.get("classification", "both")
            reason = data.get("reason", "")

            # æœ‰åŠ¹ãªå€¤ã‹ãƒã‚§ãƒƒã‚¯
            if classification not in ["materials", "methods", "both", "none"]:
                classification = "both"

            print(f"  > åˆ†é¡çµæœ: {classification}")
            print(f"  > ç†ç”±: {reason}")

        except Exception as e:
            print(f"  > âš ï¸ åˆ†é¡ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"  > ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: both")
            classification = "both"

        elapsed_time = time.time() - start_time
        print(f"  â±ï¸ Execution Time: {elapsed_time:.4f} sec")
        return {"focus_classification": classification}

    def _generate_multi_axis_queries_node(self, state: AgentState):
        """3è»¸ã‚¯ã‚¨ãƒªç”Ÿæˆãƒãƒ¼ãƒ‰ï¼ˆv3.1.0ï¼‰

        ææ–™è»¸ã€æ–¹æ³•è»¸ã€ç·åˆè»¸ã®ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã™ã‚‹
        """
        start_time = time.time()
        evaluation_mode = state.get("evaluation_mode", False)

        if evaluation_mode:
            print("\n--- ğŸ§  [3/6] 3è»¸ã‚¯ã‚¨ãƒªç”Ÿæˆ ---")
        else:
            print("--- ğŸ§  [3/7] 3è»¸ã‚¯ã‚¨ãƒªç”Ÿæˆ ---")

        focus_class = state.get('focus_classification', 'none')
        instruction = state.get('user_focus_instruction', '')

        # ææ–™è»¸ã«é‡ç‚¹æŒ‡ç¤ºã‚’é©ç”¨ã™ã‚‹ã‹ã©ã†ã‹
        apply_focus_to_material = focus_class in ["materials", "both"]
        # æ–¹æ³•è»¸ã«é‡ç‚¹æŒ‡ç¤ºã‚’é©ç”¨ã™ã‚‹ã‹ã©ã†ã‹
        apply_focus_to_method = focus_class in ["methods", "both"]

        material_instruction = instruction if apply_focus_to_material else ""
        method_instruction = instruction if apply_focus_to_method else ""

        queries = {}

        # ææ–™è»¸ã‚¯ã‚¨ãƒªç”Ÿæˆ
        try:
            print("  ğŸ“¦ ææ–™è»¸ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆä¸­...")
            material_prompt = self._get_prompt("material_query_generation").format(
                normalized_materials=state.get('normalized_materials', ''),
                user_focus_instruction=material_instruction or "ç‰¹ã«ãªã—"
            )
            response = self.search_llm.invoke(material_prompt)
            content = self._extract_json_from_response(response.content.strip())
            data = json.loads(content)
            queries["material"] = data.get("query", state.get('normalized_materials', ''))
            print(f"    > {queries['material'][:80]}...")
        except Exception as e:
            print(f"    > âš ï¸ ææ–™è»¸ã‚¯ã‚¨ãƒªç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            queries["material"] = state.get('normalized_materials', '')

        # æ–¹æ³•è»¸ã‚¯ã‚¨ãƒªç”Ÿæˆ
        try:
            print("  ğŸ”§ æ–¹æ³•è»¸ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆä¸­...")
            # v3.2.0: ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ 
            materials_for_method = state.get('normalized_materials', '')
            methods_input = state.get('input_methods', '')
            print(f"    [DEBUG] ææ–™æƒ…å ±: {materials_for_method[:100]}..." if materials_for_method else "    [DEBUG] ææ–™æƒ…å ±: ãªã—")
            print(f"    [DEBUG] æ–¹æ³•å…¥åŠ›: {methods_input[:100]}..." if methods_input else "    [DEBUG] æ–¹æ³•å…¥åŠ›: ãªã—")

            method_prompt = self._get_prompt("method_query_generation").format(
                normalized_materials=materials_for_method,  # v3.2.0: ææ–™æƒ…å ±ã‚’è¿½åŠ 
                input_methods=methods_input,
                user_focus_instruction=method_instruction or "ç‰¹ã«ãªã—"
            )
            print(f"    [DEBUG] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(method_prompt)}æ–‡å­—")

            response = self.search_llm.invoke(method_prompt)
            content = self._extract_json_from_response(response.content.strip())
            print(f"    [DEBUG] LLMå¿œç­”: {content[:200]}...")
            data = json.loads(content)
            queries["method"] = data.get("query", state.get('input_methods', ''))
            print(f"    > {queries['method'][:80]}...")
        except Exception as e:
            print(f"    > âš ï¸ æ–¹æ³•è»¸ã‚¯ã‚¨ãƒªç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            queries["method"] = state.get('input_methods', '')

        # ç·åˆè»¸ã‚¯ã‚¨ãƒªç”Ÿæˆ
        try:
            print("  ğŸ¯ ç·åˆè»¸ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆä¸­...")
            combined_prompt = self._get_prompt("combined_query_generation").format(
                input_purpose=state.get('input_purpose', ''),
                normalized_materials=state.get('normalized_materials', ''),
                input_methods=state.get('input_methods', ''),
                user_focus_instruction=instruction or "ç‰¹ã«ãªã—"
            )
            response = self.search_llm.invoke(combined_prompt)
            content = self._extract_json_from_response(response.content.strip())
            data = json.loads(content)
            combined_queries = data.get("queries", [])
            queries["combined"] = " ".join(combined_queries) if combined_queries else f"{state.get('input_purpose', '')} {state.get('normalized_materials', '')} {state.get('input_methods', '')}"
            print(f"    > {queries['combined'][:80]}...")
        except Exception as e:
            print(f"    > âš ï¸ ç·åˆè»¸ã‚¯ã‚¨ãƒªç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            queries["combined"] = f"{state.get('input_purpose', '')} {state.get('normalized_materials', '')} {state.get('input_methods', '')}"

        elapsed_time = time.time() - start_time
        print(f"  â±ï¸ Execution Time: {elapsed_time:.4f} sec")

        return {
            "material_query": queries["material"],
            "method_query": queries["method"],
            "combined_query": queries["combined"]
        }

    def _multi_axis_search_node(self, state: AgentState):
        """3è»¸æ¤œç´¢å®Ÿè¡Œãƒãƒ¼ãƒ‰ï¼ˆv3.2.0: 2ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ + è»¸åˆ¥æ¤œç´¢æ–¹å¼å¯¾å¿œï¼‰

        å„è»¸ã§ç‹¬ç«‹ã—ã¦æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹
        - ææ–™è»¸: materials_methods_collectionã‚’BM25ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
        - æ–¹æ³•è»¸: materials_methods_collectionã‚’ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢
        - ç·åˆè»¸: combined_collectionã‚’ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢
        """
        start_time = time.time()
        evaluation_mode = state.get("evaluation_mode", False)
        rerank_position = state.get("rerank_position", self.rerank_position)
        rerank_enabled = state.get("rerank_enabled", self.rerank_enabled)

        if evaluation_mode:
            print("\n--- ğŸ” [4/6] 3è»¸æ¤œç´¢å®Ÿè¡Œï¼ˆ2ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ + è»¸åˆ¥æ¤œç´¢æ–¹å¼ï¼‰---")
        else:
            print("--- ğŸ” [4/7] 3è»¸æ¤œç´¢å®Ÿè¡Œï¼ˆ2ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ + è»¸åˆ¥æ¤œç´¢æ–¹å¼ï¼‰---")

        hybrid_alpha = state.get("hybrid_alpha", self.hybrid_alpha)

        results = {}

        # v3.2.0: 2ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ§‹æˆ
        # ææ–™è»¸ã¨æ–¹æ³•è»¸ã¯åŒã˜materials_methodsã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨
        axis_vectorstores = {
            "material": self.vectorstores["materials_methods"] if self.vectorstores else self.vectorstore,
            "method": self.vectorstores["materials_methods"] if self.vectorstores else self.vectorstore,
            "combined": self.vectorstores["combined"] if self.vectorstores else self.vectorstore
        }

        # v3.2.0: è»¸åˆ¥æ¤œç´¢æ–¹å¼ï¼ˆconfig.AXIS_SEARCH_MODESã‹ã‚‰å–å¾—ï¼‰
        axis_search_modes = {
            "material": config.AXIS_SEARCH_MODES.get("material", "keyword"),   # BM25ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
            "method": config.AXIS_SEARCH_MODES.get("method", "semantic"),      # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢
            "combined": config.AXIS_SEARCH_MODES.get("combined", "semantic")   # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢
        }

        # å„è»¸ã§æ¤œç´¢ã‚’å®Ÿè¡Œ
        for axis, query in [
            ("material", state.get("material_query", "")),
            ("method", state.get("method_query", "")),
            ("combined", state.get("combined_query", ""))
        ]:
            axis_label = {"material": "ææ–™", "method": "æ–¹æ³•", "combined": "ç·åˆ"}[axis]
            target_vectorstore = axis_vectorstores[axis]
            search_mode = axis_search_modes[axis]

            # v3.2.0: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åã¨æ¤œç´¢æ–¹å¼ã‚’è¡¨ç¤º
            collection_name = target_vectorstore._collection.name if hasattr(target_vectorstore, '_collection') else "unknown"
            mode_label = {"keyword": "BM25ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "semantic": "ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯", "hybrid": "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰"}.get(search_mode, search_mode)
            print(f"\n  {'='*70}")
            print(f"  ğŸ“Š {axis_label}è»¸æ¤œç´¢ (ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {collection_name}, æ–¹å¼: {mode_label})")
            print(f"  {'='*70}")

            # v3.1.2: æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’çœç•¥ã›ãšã«è¡¨ç¤º
            print(f"  ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª:")
            print(f"     {query}")

            if not query:
                print(f"    > ã‚¯ã‚¨ãƒªãŒç©ºã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                results[axis] = []
                continue

            try:
                # v3.2.0: è»¸åˆ¥æ¤œç´¢æ–¹å¼ã‚’é©ç”¨ã—ãŸæ¤œç´¢
                search_results = self._search_with_synonym_expansion(
                    vectorstore=target_vectorstore,
                    query=query,
                    search_mode=search_mode,  # è»¸åˆ¥ã®æ¤œç´¢æ–¹å¼ã‚’ä½¿ç”¨
                    hybrid_alpha=hybrid_alpha,
                    k=config.VECTOR_SEARCH_K
                )

                print(f"  ğŸ“‹ å€™è£œæ•°: {len(search_results)}ä»¶")

                # per_axisãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€å„è»¸ã§ãƒªãƒ©ãƒ³ã‚¯
                if rerank_position == "per_axis" and rerank_enabled and search_results:
                    print(f"  ğŸ”„ ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°å®Ÿè¡Œä¸­...")
                    docs_content = [doc.page_content for doc, _ in search_results]
                    rerank_results = self.cohere_client.rerank(
                        model=config.DEFAULT_RERANK_MODEL,
                        query=query,
                        documents=docs_content,
                        top_n=min(config.RERANK_TOP_N, len(docs_content))
                    )
                    # ãƒªãƒ©ãƒ³ã‚¯çµæœã§ä¸¦ã³æ›¿ãˆ
                    reranked = []
                    for r in rerank_results.results:
                        original_doc = search_results[r.index][0]
                        reranked.append((original_doc, r.relevance_score))
                    results[axis] = reranked
                else:
                    results[axis] = search_results

                # v3.1.2: ä¸Šä½10ä»¶ã®è©³ç´°ã‚’è¡¨ç¤º
                final_results = results[axis]
                print(f"\n  ğŸ“Š {axis_label}è»¸ ä¸Šä½10ä»¶:")
                print(f"  {'-'*60}")
                seen_ids = set()
                rank_counter = 0
                for doc, score in final_results:
                    note_id = doc.metadata.get('note_id', doc.metadata.get('source', 'unknown'))
                    if note_id in seen_ids:
                        continue
                    seen_ids.add(note_id)
                    rank_counter += 1
                    print(f"  Rank {rank_counter:2d} | Score: {score:.6f} | ãƒãƒ¼ãƒˆID: {note_id}")
                    if rank_counter >= 10:
                        break
                print(f"  {'-'*60}")

            except Exception as e:
                print(f"    > âš ï¸ {axis_label}è»¸æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
                results[axis] = []

        elapsed_time = time.time() - start_time
        print(f"  â±ï¸ Execution Time: {elapsed_time:.4f} sec")

        return {
            "material_axis_results": results.get("material", []),
            "method_axis_results": results.get("method", []),
            "combined_axis_results": results.get("combined", [])
        }

    def _keyword_search_on_vectorstore(self, vectorstore, query: str, k: int = 30) -> List[tuple]:
        """æŒ‡å®šã•ã‚ŒãŸvectorstoreã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆv3.1.1è¿½åŠ ï¼‰"""
        import math
        from collections import Counter
        from langchain_core.documents import Document

        collection = vectorstore._collection
        all_docs = collection.get(include=["documents", "metadatas"])

        if not all_docs["documents"]:
            return []

        documents = all_docs["documents"]
        metadatas = all_docs["metadatas"]

        query_tokens = self._tokenize(query)

        k1 = 1.5
        b = 0.75

        doc_lengths = [len(self._tokenize(doc)) for doc in documents]
        avgdl = sum(doc_lengths) / len(doc_lengths) if doc_lengths else 1

        N = len(documents)
        idf = {}
        for token in query_tokens:
            df = sum(1 for doc in documents if token in doc.lower())
            idf[token] = math.log((N - df + 0.5) / (df + 0.5) + 1)

        scores = []
        for i, doc in enumerate(documents):
            doc_tokens = self._tokenize(doc)
            doc_len = len(doc_tokens)
            term_freq = Counter(doc_tokens)

            score = 0
            for token in query_tokens:
                if token in term_freq:
                    tf = term_freq[token]
                    numerator = tf * (k1 + 1)
                    denominator = tf + k1 * (1 - b + b * doc_len / avgdl)
                    score += idf.get(token, 0) * numerator / denominator

            doc_obj = Document(
                page_content=doc,
                metadata=metadatas[i] if metadatas else {}
            )
            scores.append((doc_obj, score))

        scores.sort(key=lambda x: x[1], reverse=True)
        return scores[:k]

    def _hybrid_search_on_vectorstore(self, vectorstore, query: str, alpha: float, k: int = 30) -> List[tuple]:
        """æŒ‡å®šã•ã‚ŒãŸvectorstoreã§ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆv3.1.1è¿½åŠ ï¼‰"""
        semantic_results = vectorstore.similarity_search_with_relevance_scores(query, k=k)
        keyword_results = self._keyword_search_on_vectorstore(vectorstore, query, k=k)

        doc_scores = {}

        if semantic_results:
            max_semantic = max(score for _, score in semantic_results)
            min_semantic = min(score for _, score in semantic_results)
            range_semantic = max_semantic - min_semantic if max_semantic != min_semantic else 1

            for doc, score in semantic_results:
                normalized_score = (score - min_semantic) / range_semantic if range_semantic > 0 else 0.5
                doc_id = doc.metadata.get('source', doc.metadata.get('note_id', doc.page_content[:50]))
                if doc_id not in doc_scores:
                    doc_scores[doc_id] = {'doc': doc, 'semantic': 0, 'keyword': 0}
                doc_scores[doc_id]['semantic'] = normalized_score

        if keyword_results:
            max_keyword = max(score for _, score in keyword_results)
            min_keyword = min(score for _, score in keyword_results)
            range_keyword = max_keyword - min_keyword if max_keyword != min_keyword else 1

            for doc, score in keyword_results:
                normalized_score = (score - min_keyword) / range_keyword if range_keyword > 0 else 0.5
                doc_id = doc.metadata.get('source', doc.metadata.get('note_id', doc.page_content[:50]))
                if doc_id not in doc_scores:
                    doc_scores[doc_id] = {'doc': doc, 'semantic': 0, 'keyword': 0}
                doc_scores[doc_id]['keyword'] = normalized_score

        combined_results = []
        for doc_id, scores in doc_scores.items():
            combined_score = alpha * scores['semantic'] + (1 - alpha) * scores['keyword']
            combined_results.append((scores['doc'], combined_score))

        combined_results.sort(key=lambda x: x[1], reverse=True)
        return combined_results[:k]

    def _score_fusion_node(self, state: AgentState):
        """ã‚¹ã‚³ã‚¢çµ±åˆãƒãƒ¼ãƒ‰ï¼ˆv3.1.1: note_idã§ã®çµæœãƒãƒ¼ã‚¸å¯¾å¿œï¼‰

        å„è»¸ã®æ¤œç´¢çµæœã‚’çµ±åˆã—ã¦æœ€çµ‚ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’ç”Ÿæˆã™ã‚‹
        - å„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®æ¤œç´¢çµæœã‚’note_idã§ãƒãƒ¼ã‚¸
        - åŒã˜ãƒãƒ¼ãƒˆãŒè¤‡æ•°è»¸ã§ãƒ’ãƒƒãƒˆã—ãŸå ´åˆã€å„è»¸ã®ã‚¹ã‚³ã‚¢ã‚’çµ±åˆ
        """
        start_time = time.time()
        evaluation_mode = state.get("evaluation_mode", False)
        fusion_method = state.get("fusion_method", self.fusion_method)
        axis_weights = state.get("axis_weights", self.axis_weights)
        rerank_position = state.get("rerank_position", self.rerank_position)
        rerank_enabled = state.get("rerank_enabled", self.rerank_enabled)

        if evaluation_mode:
            print("\n--- ğŸ”€ [5/6] ã‚¹ã‚³ã‚¢çµ±åˆï¼ˆnote_idã§ãƒãƒ¼ã‚¸ï¼‰---")
        else:
            print("--- ğŸ”€ [5/7] ã‚¹ã‚³ã‚¢çµ±åˆï¼ˆnote_idã§ãƒãƒ¼ã‚¸ï¼‰---")

        print(f"  > çµ±åˆæ–¹å¼: {fusion_method}")
        print(f"  > ã‚¦ã‚¨ã‚¤ãƒˆ: ææ–™={axis_weights.get('material', 0.3)}, æ–¹æ³•={axis_weights.get('method', 0.4)}, ç·åˆ={axis_weights.get('combined', 0.3)}")

        # å„è»¸ã®çµæœã‚’å–å¾—
        material_results = state.get("material_axis_results", [])
        method_results = state.get("method_axis_results", [])
        combined_results = state.get("combined_axis_results", [])

        # v3.1.1: note_idã§ãƒãƒ¼ã‚¸ã™ã‚‹ãŸã‚ã®ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒª
        # {note_id: {"docs": {axis: doc}, "scores": {axis: score}, "ranks": {axis: rank}}}
        doc_scores = {}

        for axis, results in [
            ("material", material_results),
            ("method", method_results),
            ("combined", combined_results)
        ]:
            for rank, (doc, score) in enumerate(results):
                # v3.1.1: note_idã‚’å„ªå…ˆçš„ã«ä½¿ç”¨ï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã§çµ±ä¸€ã•ã‚ŒãŸIDï¼‰
                note_id = doc.metadata.get('note_id', doc.metadata.get('source', doc.page_content[:50]))
                if note_id not in doc_scores:
                    doc_scores[note_id] = {
                        "docs": {"material": None, "method": None, "combined": None},
                        "scores": {"material": None, "method": None, "combined": None},
                        "ranks": {"material": None, "method": None, "combined": None}
                    }
                # å„è»¸ã®docã‚’ä¿å­˜ï¼ˆcombinedå„ªå…ˆã§æœ€çµ‚çš„ãªdocã‚’æ±ºå®šï¼‰
                doc_scores[note_id]["docs"][axis] = doc
                doc_scores[note_id]["scores"][axis] = score
                doc_scores[note_id]["ranks"][axis] = rank + 1  # 1-indexed

        # ã‚¹ã‚³ã‚¢çµ±åˆ
        final_scores = []
        rrf_k = config.RRF_K

        for note_id, data in doc_scores.items():
            if fusion_method == "rrf":
                # RRF (Reciprocal Rank Fusion)
                score = 0
                for axis in ["material", "method", "combined"]:
                    rank = data["ranks"][axis]
                    weight = axis_weights.get(axis, 0.33)
                    if rank is not None:
                        score += weight / (rrf_k + rank)
            else:
                # ç·šå½¢çµåˆ
                score = 0
                for axis in ["material", "method", "combined"]:
                    axis_score = data["scores"][axis]
                    weight = axis_weights.get(axis, 0.33)
                    if axis_score is not None:
                        # ã‚¹ã‚³ã‚¢ã‚’0-1ã«æ­£è¦åŒ–ï¼ˆã™ã§ã«æ­£è¦åŒ–ã•ã‚Œã¦ã„ã‚‹å‰æï¼‰
                        score += weight * axis_score

            # v3.1.1: æœ€çµ‚çš„ãªdocã¯combinedã‚’å„ªå…ˆã€ãªã‘ã‚Œã°ä»–ã®è»¸ã‹ã‚‰å–å¾—
            final_doc = data["docs"]["combined"] or data["docs"]["method"] or data["docs"]["material"]
            if final_doc:
                final_scores.append((final_doc, score, note_id))

        # ã‚¹ã‚³ã‚¢é™é †ã§ã‚½ãƒ¼ãƒˆ
        final_scores.sort(key=lambda x: x[1], reverse=True)

        # after_fusionãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€çµ±åˆå¾Œã«ãƒªãƒ©ãƒ³ã‚¯
        if rerank_position == "after_fusion" and rerank_enabled and final_scores:
            print(f"  > çµ±åˆå¾Œãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°å®Ÿè¡Œä¸­...")
            # ä¸Šä½å€™è£œã«å¯¾ã—ã¦ãƒªãƒ©ãƒ³ã‚¯
            top_candidates = final_scores[:config.RERANK_TOP_N * 2]  # ä½™è£•ã‚’æŒã£ã¦å–å¾—
            if top_candidates:
                # ã‚¯ã‚¨ãƒªã¯ç·åˆã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨
                combined_query = state.get("combined_query", "")
                docs_content = [doc.page_content for doc, _, _ in top_candidates]

                try:
                    rerank_results = self.cohere_client.rerank(
                        model=config.DEFAULT_RERANK_MODEL,
                        query=combined_query,
                        documents=docs_content,
                        top_n=min(config.RERANK_TOP_N, len(docs_content))
                    )
                    # ãƒªãƒ©ãƒ³ã‚¯çµæœã§ä¸¦ã³æ›¿ãˆ
                    reranked = []
                    for r in rerank_results.results:
                        doc, _, source_id = top_candidates[r.index]
                        reranked.append((doc, r.relevance_score, source_id))
                    final_scores = reranked
                    print(f"  > ãƒªãƒ©ãƒ³ã‚¯å¾Œ: {len(final_scores)}ä»¶")
                except Exception as e:
                    print(f"  > âš ï¸ ãƒªãƒ©ãƒ³ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

        # é‡è¤‡é™¤å»ã—ã¦UIç”¨ã®çµæœã‚’ä½œæˆ
        docs_for_ui = []
        seen_source_ids = set()
        display_limit = config.RERANK_TOP_N if evaluation_mode else config.UI_DISPLAY_TOP_N

        print(f"\n  ğŸ“Š [æœ€çµ‚ãƒ©ãƒ³ã‚­ãƒ³ã‚°]")
        print(f"  " + "="*60)

        rank_counter = 0
        for doc, score, source_id in final_scores:
            if source_id in seen_source_ids:
                continue
            seen_source_ids.add(source_id)
            rank_counter += 1

            if evaluation_mode:
                print(f"  Rank {rank_counter:2d} | Score: {score:.6f} | ãƒãƒ¼ãƒˆID: {source_id}")
            else:
                snippet = doc.page_content[:50].replace('\n', ' ')
                print(f"  Rank {rank_counter:2d} | Score: {score:.4f} | ID: {source_id} | {snippet}...")

            if rank_counter <= display_limit:
                docs_for_ui.append(f"ã€å®Ÿé¨“ãƒãƒ¼ãƒˆID: {source_id}ã€‘\n{doc.page_content}")

            if rank_counter >= config.RERANK_TOP_N:
                break

        print(f"  " + "="*60)

        if evaluation_mode:
            print(f"  âœ… è©•ä¾¡ç”¨ã«ä¸Šä½ {len(docs_for_ui)} ä»¶ã‚’è¿”å´ã—ã¾ã™ã€‚")
        else:
            print(f"  > UIå‘ã‘ã«ä¸Šä½ {len(docs_for_ui)} ä»¶ã‚’é¸æŠã—ã¾ã—ãŸã€‚")

        # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰æ™‚ã¯çµ‚äº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        if evaluation_mode:
            print("\n" + "="*80)
            print("âœ… è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰çµ‚äº† - æ¯”è¼ƒãƒãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦çµæœã‚’è¿”å´ã—ã¾ã™")
            print("="*80 + "\n")

        elapsed_time = time.time() - start_time
        print(f"  â±ï¸ Execution Time: {elapsed_time:.4f} sec")

        return {
            "retrieved_docs": docs_for_ui,
            "iteration": state.get("iteration", 0) + 1
        }

    # ===========================================
    # å…±é€šãƒãƒ¼ãƒ‰
    # ===========================================

    def _compare_node(self, state: AgentState):
        """æ¯”è¼ƒãƒ»è¦ç´„ç”Ÿæˆãƒãƒ¼ãƒ‰"""
        start_time = time.time()
        print("--- ğŸ“ [4/4] æ¯”è¼ƒãƒ»è¦ç´„ç”Ÿæˆ (Deep Analysis) ---")

        input_purpose = state.get('input_purpose')
        input_materials = state.get('normalized_materials')
        input_methods = state.get('input_methods')
        instruction = state.get('user_focus_instruction', '')

        docs_str = "\n\n".join(state.get("retrieved_docs", []))

        if not docs_str:
            print(f"  â±ï¸ Execution Time: {time.time() - start_time:.4f} sec")
            return {"messages": [HumanMessage(content="è©²å½“ã™ã‚‹ãƒãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")]}

        # ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
        prompt_template = self._get_prompt("compare")

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¤‰æ•°ã‚’åŸ‹ã‚è¾¼ã‚€
        prompt = prompt_template.format(
            input_purpose=input_purpose,
            normalized_materials=input_materials,
            input_methods=input_methods,
            user_focus_instruction=instruction,
            retrieved_docs=docs_str
        )

        # v3.0: è¦ç´„ç”Ÿæˆç”¨LLMã‚’ä½¿ç”¨
        response = self.summary_llm.invoke(prompt)

        elapsed_time = time.time() - start_time
        print(f"  â±ï¸ Execution Time: {elapsed_time:.4f} sec (using {self.summary_llm_model})")
        return {"messages": [response]}

    def _should_compare(self, state: AgentState):
        """compareãƒãƒ¼ãƒ‰ã«é€²ã‚€ã¹ãã‹ã‚’åˆ¤å®š"""
        evaluation_mode = state.get("evaluation_mode", False)
        if evaluation_mode:
            return END
        else:
            return "compare"

    def _should_use_multi_axis(self, state: AgentState):
        """3è»¸æ¤œç´¢ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        multi_axis_enabled = state.get("multi_axis_enabled", self.multi_axis_enabled)
        if multi_axis_enabled:
            return "classify_focus"
        else:
            return "generate_query"

    def _should_compare_multi_axis(self, state: AgentState):
        """3è»¸æ¤œç´¢å¾Œã«compareãƒãƒ¼ãƒ‰ã«é€²ã‚€ã¹ãã‹ã‚’åˆ¤å®š"""
        evaluation_mode = state.get("evaluation_mode", False)
        if evaluation_mode:
            return END
        else:
            return "compare"

    def _build_graph(self):
        """ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ï¼ˆv3.1.0: 3è»¸åˆ†é›¢æ¤œç´¢å¯¾å¿œï¼‰"""
        workflow = StateGraph(AgentState)

        # å…±é€šãƒãƒ¼ãƒ‰
        workflow.add_node("normalize", self._normalize_node)
        workflow.add_node("compare", self._compare_node)

        # å¾“æ¥ã®å˜ä¸€ã‚¯ã‚¨ãƒªæ¤œç´¢ãƒãƒ¼ãƒ‰
        workflow.add_node("generate_query", self._generate_query_node)
        workflow.add_node("search", self._search_node)

        # 3è»¸åˆ†é›¢æ¤œç´¢ãƒãƒ¼ãƒ‰ï¼ˆv3.1.0ï¼‰
        workflow.add_node("classify_focus", self._classify_focus_node)
        workflow.add_node("generate_multi_axis_queries", self._generate_multi_axis_queries_node)
        workflow.add_node("multi_axis_search", self._multi_axis_search_node)
        workflow.add_node("score_fusion", self._score_fusion_node)

        # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
        workflow.set_entry_point("normalize")

        # normalizeå¾Œã«3è»¸æ¤œç´¢ã‹å¾“æ¥æ¤œç´¢ã‹ã‚’åˆ†å²
        workflow.add_conditional_edges(
            "normalize",
            self._should_use_multi_axis,
            {
                "classify_focus": "classify_focus",
                "generate_query": "generate_query"
            }
        )

        # å¾“æ¥ã®æ¤œç´¢ãƒ•ãƒ­ãƒ¼
        workflow.add_edge("generate_query", "search")
        workflow.add_conditional_edges(
            "search",
            self._should_compare,
            {
                "compare": "compare",
                END: END
            }
        )

        # 3è»¸åˆ†é›¢æ¤œç´¢ãƒ•ãƒ­ãƒ¼
        workflow.add_edge("classify_focus", "generate_multi_axis_queries")
        workflow.add_edge("generate_multi_axis_queries", "multi_axis_search")
        workflow.add_edge("multi_axis_search", "score_fusion")
        workflow.add_conditional_edges(
            "score_fusion",
            self._should_compare_multi_axis,
            {
                "compare": "compare",
                END: END
            }
        )

        # æ¯”è¼ƒãƒãƒ¼ãƒ‰ã‹ã‚‰çµ‚äº†
        workflow.add_edge("compare", END)

        return workflow.compile()

    def run(self, input_data: dict, evaluation_mode: bool = False):
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œ

        Args:
            input_data: æ¤œç´¢æ¡ä»¶ï¼ˆpurpose, materials, methodsç­‰ï¼‰
            evaluation_mode: è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ï¼ˆTrue: æ¯”è¼ƒçœç•¥ã€Top10è¿”å´ã€False: é€šå¸¸å‹•ä½œï¼‰
        """
        initial_state = {
            "messages": [HumanMessage(content=json.dumps(input_data, ensure_ascii=False))],
            "input_purpose": "",
            "input_materials": "",
            "input_methods": "",
            "normalized_materials": "",
            "user_focus_instruction": "",
            "search_query": "",
            "retrieved_docs": [],
            "iteration": 0,
            "evaluation_mode": evaluation_mode,
            # v3.0.1: æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰è¨­å®š
            "search_mode": self.search_mode,
            "hybrid_alpha": self.hybrid_alpha,
            # v3.1.0: 3è»¸åˆ†é›¢æ¤œç´¢è¨­å®š
            "multi_axis_enabled": self.multi_axis_enabled,
            "focus_classification": "",
            "fusion_method": self.fusion_method,
            "axis_weights": self.axis_weights,
            "rerank_position": self.rerank_position,
            "rerank_enabled": self.rerank_enabled,
            # 3è»¸æ¤œç´¢çµæœ
            "material_query": "",
            "method_query": "",
            "combined_query": "",
            "material_axis_results": [],
            "method_axis_results": [],
            "combined_axis_results": []
        }

        result = self.graph.invoke(initial_state)
        return result
